# rag_pipeline.py

import os
import json
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# --- .env dosyasÄ±nÄ± yÃ¼kle ---
load_dotenv()

# --- Google API anahtarÄ± ---
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    raise ValueError("GOOGLE_API_KEY .env dosyasÄ±nda bulunamadÄ±!")
os.environ["GOOGLE_API_KEY"] = api_key

# --- 1. Veri YÃ¼kleme ---
with open("yapay_zeka_chunks_clean2.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

docs = [
    Document(page_content=c["content"], metadata={"page": c.get("page", None)})
    for c in chunks
]

# --- 2. Embedding ve VektÃ¶r VeritabanÄ± ---
embedding = HuggingFaceEmbeddings(model_name="paraphrase-multilingual-MiniLM-L12-v2")

# EÄŸer veritabanÄ± mevcutsa yeniden embedding yapma
if not os.path.exists("chroma_db"):
    print("ğŸ” Yeni embedding veritabanÄ± oluÅŸturuluyor...")
    vectorstore = Chroma.from_documents(docs, embedding, persist_directory="chroma_db")
    vectorstore.persist()
else:
    print("ğŸ“ Var olan embedding veritabanÄ± yÃ¼kleniyor...")
    vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embedding)

retriever = vectorstore.as_retriever(search_kwargs={"k": 15})

# --- 3. LLM (Gemini 2.5 Pro) ---
llm = GoogleGenerativeAI(model="gemini-2.5-pro")

# --- 4. Prompt Åablonu ---
prompt_template = """
Sen Bilim ve Teknik dergisinin Ocak 2018 sayÄ±sÄ±ndaki 'Yapay ZekÃ¢' makalesine dayalÄ± bir asistansÄ±n.
AÅŸaÄŸÄ±daki baÄŸlamdan yararlanarak soruya TÃ¼rkÃ§e, aÃ§Ä±k ve doÄŸru bir yanÄ±t ver.
EÄŸer baÄŸlamda cevap varsa, sadece baÄŸlamdan faydalanarak cevap ver.
EÄŸer baÄŸlamda hiÃ§bir bilgi yoksa "Bu bilgi makalede yer almÄ±yor." de.


BaÄŸlam:
{context}

Soru:
{question}
"""

prompt = PromptTemplate.from_template(prompt_template)

# --- 5. RetrievalQA Zinciri ---
llm = GoogleGenerativeAI(model="gemini-2.5-pro")
retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},
)

print("âœ… RAG pipeline baÅŸarÄ±yla yÃ¼klendi!")

# --- 6. Export edilecek nesneler ---
__all__ = ["llm", "retriever", "qa_chain"]
