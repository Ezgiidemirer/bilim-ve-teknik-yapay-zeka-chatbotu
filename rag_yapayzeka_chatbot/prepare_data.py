# 1_prepare_data.py
# LangChain 0.2+ uyumlu: PDF -> Temizleme -> Chunk -> JSON

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import json
import re

# === Dosya yollarÄ± ===
pdf_path = "yazi.pdf"
output_json = "yapay_zeka_chunks_clean2.json"

# === YardÄ±mcÄ± temizleme fonksiyonu ===
def clean_text(text):
    # PDF sayfa altlÄ±klarÄ±nÄ±, satÄ±r sonlarÄ±nÄ± ve gereksiz boÅŸluklarÄ± temizle
    text = re.sub(r"\d{2}_\d{2}_.*?indd.*?\d{2}:\d{2}", " ", text)  # sayfa dip notu
    text = re.sub(r"-\n", "", text)  # tireyle bÃ¶lÃ¼nen kelimeleri birleÅŸtir
    text = re.sub(r"\n+", "\n", text)  # fazla boÅŸ satÄ±rlarÄ± azalt
    text = re.sub(r"\s+", " ", text)  # fazla boÅŸluklarÄ± dÃ¼zelt

    # ÅapkalÄ± aâ€™yÄ± dÃ¼zelt
    text = text.replace("Ã¢", "a")

    return text.strip()

# === PDF yÃ¼kleme ===
print("ğŸ“˜ PDF yÃ¼kleniyor...")
loader = PyPDFLoader(pdf_path)
documents = loader.load()
print(f"ğŸ“„ {len(documents)} sayfa bulundu.")

# === Temizleme ===
for doc in documents:
    doc.page_content = clean_text(doc.page_content)

# === Chunk (parÃ§a) oluÅŸturma ===
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,        # daha uzun parÃ§a
    chunk_overlap=200,     # baÄŸlam koruma
    separators=["\n\n", "\n", ". ", " "]  # anlamlÄ± bÃ¶lmeler
)

texts = text_splitter.split_documents(documents)

# === KÄ±sa chunkâ€™larÄ± filtrele ===
filtered_texts = [
    doc for doc in texts if len(doc.page_content.strip())>120
]
# === JSONâ€™a kaydetme ===
data = [
    {"page": doc.metadata.get("page", 0), "content": doc.page_content}
    for doc in filtered_texts
]

with open(output_json, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print(f"âœ… {len(data)} temiz metin parÃ§asÄ± oluÅŸturuldu ve '{output_json}' dosyasÄ±na kaydedildi.")
